# 8. Run example data (CPJUMP1)



1. For the Broad Imaging Platform Group, we use the virtual DGX machine to access GPU for DP related work. DGX does not have sudo, so use conda and pip installation instructions instead.
    * On the DGX, you should create a personal working directory in <code><em>/dgx1nas1/storage/ </em></code>and go with that for creating your project directories, e.g., <code>/dgx1nas1/storage/Yu</code>
    * To link a directory on DGX. <code><em>ln -s TargetDirectory WhereToLinkTo</em></code> (i.e.,<code> ln -s /dgx1nas1/storage/Yu/ /home/jupyter-yhan@broadinstitut-576b7/Yu</code>)

	Note: CPJUMP1 data is partially publicly available, all the instructions below apply to   global users, except the DGX specific instructions.



2. Download Your Own Dataset
    1. In home directory, use command <code><em>mkdir .aws to make a hidden directory, </em></code>
    2. <em>In the .aws directory, create two files. </em>

        <em>      i.<strong>      credentials</strong> (this needs to be the broad imaging credentials)</em>


                    ```
                    [jumpcpuser]
                    aws_access_key_id=
                    aws_secret_access_key=
                    ```



        _    j.          **config**_


            ```
            [profile jump-cp-role]
            role_arn = arn:aws:iam::385009899373:role/add-jump-cp-role
            source_profile = jumpcpuser
            ```


    3. _To list all the files in the CPJUMP1 s3 _

            ```
            aws s3 ls --no-sign-request s3://cellpainting-gallery/jump-pilot/source_4/images/2020_11_04_CPJUMP1/images/
            ```



          d.  _To copy image files to your desired directory, first leave the .aws directory _and go to main directory


                ```
                aws s3 cp --recursive  --no-sign-request  s3://cellpainting-gallery/jump-pilot/source_4/images/2020_11_04_CPJUMP1/images/BR00117012__2020-11-08T14_58_34-Measurement1/ Targetd_Directory (e.g., Yu/inputs/images)
                ```


3.  Generate locations using SQLite files.

        SQLite files are located in  `s3://cellpainting-gall`


        ```
        ery/jump-pilot/source_4/workspace/backend/
        ```


    4. Put the SQLite files in directory

                 `/dgx1nas1/storage/data/Yu/scripts`

    5. Put the extract_locations.py in  `/dgx1nas1/storage/data/Yu/scripts`

                '''


                Script to extract locations from CellProfiler


                '''


                import sqlite3


                import argparse


                import csv


                from collections import defaultdict


                import os.path


                from os import makedirs


                parser = argparse.ArgumentParser(


                    description='Extract locations from SQLite database generated by CellProfiler')


                parser.add_argument('dbpath', help='.sqlite file location')


                parser.add_argument(


                    'csvdir', help='destination folder to write locations in csv format')


                args = parser.parse_args()


                QUERY = '''


                SELECT


                    Metadata_Plate,


                    Metadata_Well,


                    Metadata_Site,


                    FileName_OrigRNA AS RNA,


                    FileName_OrigER AS ER,


                    FileName_OrigAGP AS AGP,


                    FileName_OrigMito AS Mito,


                    FileName_OrigDNA as DNA,


                    CAST(Nuclei_Location_Center_X AS INTEGER),


                    CAST(Nuclei_Location_Center_Y AS INTEGER)


                FROM


                    Nuclei


                JOIN Image ON


                    Image.TableNumber = Nuclei.TableNumber


                '''


                locations = defaultdict(list)


                with sqlite3.connect(args.dbpath) as conn:


                    cur = conn.cursor()


                    for row in cur.execute(QUERY):


                        plate_id, well_id, site_id, rna, er, agp, mito, dna, xpos, ypos = row


                        csvfile = f'{well_id}-{site_id}-Nuclei.csv'


                        locations[csvfile].append((xpos, ypos))


                makedirs(args.csvdir, exist_ok=True)


                header = 'Nuclei_Location_Center_X', 'Nuclei_Location_Center_Y'


                for csvfile, values in locations.items():


                    csvpath = os.path.join(args.csvdir, csvfile)


                    with open(csvpath, 'w', newline='', encoding='utf-8') as fpointer:


                        writer = csv.writer(fpointer)


                        writer.writerow(header)


                        for row in values:


                            writer.writerow(row)

    6. To execute the script_ python extract_locations.py._  

        ```
        SQLite_file_path_directory WheretheSQLiteFileIsLocated TargetDirectory. (i.e., python extract_locations.py /dgx1nas1/storage/data/Yu/scripts/BR00117012.sqlite /dgx1nas1/storage/data/Yu/inputs/locations/BR00117012)
        ```



        You should then see a bunch of csv location files generated in the same directory where the SQLite file is.

4. Using Dbeaver to find information for the index csv file
1. In order to generate the index.csv file, first download Dbeaver or DBBrowser, as well as the SQLite file to your local computer so you can read it in Dbeaver or DBBrowser.
2. Run this script below in Dbeaver or DBBrowser to extract information needed for the index.csv file.

                SELECT


                  Metadata_Plate AS Metadata_Plate,


                  Metadata_Well AS Metadata_Well,


                  Metadata_Site AS Metadata_Site,


                  FileName_OrigRNA AS RNA,


                  FileName_OrigER AS ER,


                  FileName_OrigAGP AS AGP,


                  FileName_OrigMito AS Mito,


                  FileName_OrigDNA as DNA


                FROM Image


Note: the index.csv file generated by DBBroswer’s column entries will look like `e.g., ro1co1fo1….tiff. `You need to manually add the plate number and the <span style="text-decoration:underline;">Image</span> directory in front of every entry. The final column entry should be


```
e.g., BR00116991/Images/ro1co1fo1….tiff
```


5. Example Config json file

   The model below is a pre-trained ImageNet.


                {


                    "dataset": {


                        "images": {


                            "bits": 16,


                            "channels": [


                                "DNA",


                                "ER",


                                "RNA",


                                "AGP",


                                "Mito"


                            ],


                            "file_format": "tif",


                            "height": 1080,


                            "width": 1080


                        },


                        "locations": {


                            "area_coverage": 0.75,


                            "box_size": 96,


                            "mask_objects": false,


                            "mode": "single_cells"


                        },


                        "metadata": {


                            "control_value": "DMSO_0.0",


                            "label_field": "Metadata_Plate"


                        }


                    },


                    "prepare": {


                        "compression": {


                            "implement": false,


                            "scaling_factor": 1.0


                        },


                        "illumination_correction": {


                            "down_scale_factor": 4,


                            "median_filter_size": 24


                        }


                    },


                    "profile": {


                        "batch_size": 1024,


                        "checkpoint": "efficientnetb0_notop.h5",


                        "feature_layer": "avg_pool",


                        "use_pretrained_input_size": 224


                    },


                    "train": {


                        "model": {


                            "crop_generator": "repeat_channel_crop_generator",


                            "epochs": 100,


                            "initialization": "ImageNet",


                            "lr_schedule": "cosine",


                            "metrics": [


                                "accuracy",


                                "top_k"


                            ],


                            "name": "efficientnet",


                            "params": {


                                "batch_size": 64,


                                "conv_blocks": 0,


                                "feature_dim": 256,


                                "learning_rate": 0.005,


                                "pooling": "avg",


                		"label_smoothing": 0.9


                            }


                        },


                        "partition": {


                            "split_field": "Allele_Replicate",


                            "targets": [


                                "Metadata_Plate"


                            ],


                            "training_values": [


                                0


                            ],


                            "validation_values": [


                                1


                            ]


                        },


                        "sampling": {


                            "cache_size": 15000,


                            "factor": 1,


                            "workers": 4


                        },


                        "validation": {


                            "batch_size": 32,


                            "frame": "val",


                            "frequency": 2,


                            "sample_first_crops": true,


                            "top_k": 5


                        }


                    }


                }
